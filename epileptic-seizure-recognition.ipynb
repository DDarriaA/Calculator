{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6062139,"sourceType":"datasetVersion","datasetId":3469157},{"sourceId":11527480,"sourceType":"datasetVersion","datasetId":7229889},{"sourceId":11612474,"sourceType":"datasetVersion","datasetId":7283997},{"sourceId":11612754,"sourceType":"datasetVersion","datasetId":7284223},{"sourceId":11705298,"sourceType":"datasetVersion","datasetId":7347246},{"sourceId":11800662,"sourceType":"datasetVersion","datasetId":7410692},{"sourceId":11866870,"sourceType":"datasetVersion","datasetId":7457120},{"sourceId":374374,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":309529,"modelId":329905},{"sourceId":401753,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":328683,"modelId":349518}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sectiunea 1 - Importarea bibliotecilor și definirea canalelor","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n!pip install wfdb\n!pip install mne\n\nimport os\nimport matplotlib.pyplot as plt\n#import pyedflib\nimport wfdb #WFDB (Waveform Database) package\nimport glob\nimport random\nimport gc\nimport mne\nfrom scipy.signal import find_peaks\nimport re\nimport tqdm\nimport logging","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:12:55.177123Z","iopub.execute_input":"2025-05-20T17:12:55.177416Z","iopub.status.idle":"2025-05-20T17:13:05.683461Z","shell.execute_reply.started":"2025-05-20T17:12:55.177394Z","shell.execute_reply":"2025-05-20T17:13:05.682605Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Channels of bipolar montage, there are used 18 out of 23:","metadata":{}},{"cell_type":"code","source":"ch_labels = ['FP1-F7', 'F7-T7', 'T7-P7', 'P7-O1', 'FP1-F3', 'F3-C3', 'C3-P3','P3-O1',\n           'FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2', 'FP2-F8', 'F8-T8', 'T8-P8', 'P8-O2',\n           'FZ-CZ', 'CZ-PZ']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:14:25.742832Z","iopub.execute_input":"2025-05-20T17:14:25.743580Z","iopub.status.idle":"2025-05-20T17:14:25.747365Z","shell.execute_reply.started":"2025-05-20T17:14:25.743556Z","shell.execute_reply":"2025-05-20T17:14:25.746800Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Secțiunea 2 - Manipularea datelor","metadata":{}},{"cell_type":"markdown","source":"I extract the patients IDs","metadata":{}},{"cell_type":"code","source":"import glob\npath = '/kaggle/input/seizure-epilepcy-chb-mit-eeg-dataset-pediatric/chb-mit-scalp-eeg-database-1.0.0'\n\nfolders = sorted(glob.glob(path+'/*/'))\nn_patient = [m[-2:] for m in [l.rsplit('/', 2)[-2] for l in folders]]\n\nprint(*n_patient)#the asterix * is for no brackets and commas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:14:28.974411Z","iopub.execute_input":"2025-05-20T17:14:28.975045Z","iopub.status.idle":"2025-05-20T17:14:29.079834Z","shell.execute_reply.started":"2025-05-20T17:14:28.975019Z","shell.execute_reply":"2025-05-20T17:14:29.079082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I randomise the patients and select the first 19 for training and last 5 for testing","metadata":{}},{"cell_type":"code","source":"import random\nrandom.seed(2023)\n\nratio_train = 0.8\ntrain_patient_str = sorted(random.sample(n_patient, round(ratio_train*len(n_patient))))\ntest_patient_str = sorted([l for l in n_patient if l not in train_patient_str])\nprint('Train PT: ', *train_patient_str)\nprint('Test PT: ', *test_patient_str)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:14:30.819331Z","iopub.execute_input":"2025-05-20T17:14:30.819626Z","iopub.status.idle":"2025-05-20T17:14:30.824855Z","shell.execute_reply.started":"2025-05-20T17:14:30.819601Z","shell.execute_reply":"2025-05-20T17:14:30.823895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Shows how many files are in total. (train, test)","metadata":{}},{"cell_type":"code","source":"files_train = []\nfor l in train_patient_str:\n    files_train = files_train + glob.glob(path+'/chb{}/*.edf'.format(l))\n\nfiles_test = []\nfor l in test_patient_str:\n    files_test = files_test + glob.glob(path+'/chb{}/*.edf'.format(l))\n    \nlen(files_train), len(files_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:14:32.825185Z","iopub.execute_input":"2025-05-20T17:14:32.825761Z","iopub.status.idle":"2025-05-20T17:14:33.284117Z","shell.execute_reply.started":"2025-05-20T17:14:32.825738Z","shell.execute_reply":"2025-05-20T17:14:33.283432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Secțiunea 3 - Preprocesarea","metadata":{}},{"cell_type":"code","source":"mne.set_log_level(verbose='ERROR') #show only error messages","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:05:19.919981Z","iopub.execute_input":"2025-05-20T17:05:19.921212Z","iopub.status.idle":"2025-05-20T17:05:19.926429Z","shell.execute_reply.started":"2025-05-20T17:05:19.921171Z","shell.execute_reply":"2025-05-20T17:05:19.925129Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Secțiunea 3.1 - Extragerea semnalelor și atribuirea etichetelor ","metadata":{}},{"cell_type":"code","source":"#creates a logging system information about processed files into a file called 'read_files.log'\nlogger = logging.getLogger(__name__)\nfh = logging.FileHandler('read_files.log')\nlogger.addHandler(fh)\n\n#windows overlap 50%\ntime_window = 8 # 8-second time window\ntime_step = 4 # slides forward by 4 seconds\n\np = 0.01  \ncounter = 0 #how many eeg segments we have in total\n#incarcam \nfor temp_f in files_train: #temp_f = fisier .edf individual\n    temp_edf =  mne.io.read_raw_edf(temp_f) #citeste fiserul edf si creeaza un obiect de tip raw\n    temp_labels = temp_edf.ch_names # lista canalelor EEG\n    if sum([any([0 if re.match(c, l)==None else 1 for l in temp_edf.ch_names]) for c in ch_labels])==len(ch_labels): #verifies if all channels exist\n        time_window = 8\n        time_step = 4\n        fs = int(1/(temp_edf.times[1]-temp_edf.times[0]))\n        step_window = time_window*fs #step-window cati pasi sunt intr-o fereastra de 8 secunde\n        step = time_step*fs #cât \"alunecă\" fereastra (4 sec * 256 Hz = 1024 eșantioane)\n        #temp_is_sz este un array de 0 și 1 care indică pentru fiecare eșantion dacă se află sau nu se află într-o criză. \n        temp_is_sz = np.zeros((temp_edf.n_times,)) #array cu val 0 pt tot semnalul\n        \n        #Marcheză porțiunile de semnal în care apar crizele, setând 1 în array-ul temp_is_sz, adica fisierele .edf.seizures\n        if os.path.exists(temp_f+'.seizures'):\n            temp_annotation = wfdb.rdann(temp_f, 'seizures')\n            for i in range(int(temp_annotation.sample.size/2)):\n                temp_is_sz[temp_annotation.sample[i*2]:temp_annotation.sample[i*2+1]]=1#Marchează cu 1 toate eșantioanele dintre început și sfârșit ca fiind în criză.\n                \n        #vector cu proportia de criza\n        temp_len = temp_edf.n_times\n        temp_is_sz_ind = np.array( #temp_is_sz_ind va avea valori între 0 și 1 (0 înseamnă nicio criză, 1 înseamnă criză 100% pe toată fereastra)\n            [temp_is_sz[i*step:i*step+step_window].sum()/step_window for i in range((temp_len-step_window)//step)]#cat intre 0 si 1 fereastra e in criza\n        )\n\n        #calculează câte segmente cu/și fără crize vor fi extrase\n        temp_0_sample_size = round(p*np.where(temp_is_sz_ind==0)[0].size)\n        temp_1_sample_size = np.where(temp_is_sz_ind>0)[0].size\n        counter = counter + temp_0_sample_size + temp_1_sample_size\n    temp_edf.close()\n    \n#creez arrays dupa ce am calculat dimensiunile totale\narray_signals = np.zeros((counter, len(ch_labels), step_window), dtype=np.float32)\narray_is_sz = np.zeros(counter, dtype=bool)\nsource_files = []\n\n# citește din nou fișierele și extrage efectiv semnalele\ncounter = 0\nfor n, temp_f in enumerate(tqdm.tqdm(files_train)):\n    to_log = 'No. {}: Reading. '.format(n)\n    temp_edf =  mne.io.read_raw_edf(temp_f)\n    temp_labels = temp_edf.ch_names\n    n_label_match = sum([any([0 if re.match(c, l)==None else 1 for l in temp_edf.ch_names]) for c in ch_labels])\n    if n_label_match==len(ch_labels):\n        ch_mapping = {sorted([l for l in temp_edf.ch_names if re.match(c, l)!=None ])[0]:c for c in ch_labels}\n        temp_edf.rename_channels(ch_mapping)\n        #temp_edf = temp_edf.pick(ch_labels)\n\n        temp_is_sz = np.zeros((temp_edf.n_times,))\n        temp_signals = temp_edf.get_data(picks=ch_labels)*1e6\n        #marcheaza din nou crizele\n        if os.path.exists(temp_f+'.seizures'):\n            to_log = to_log+'sz exists.'\n            temp_annotation = wfdb.rdann(temp_f, 'seizures')\n            for i in range(int(temp_annotation.sample.size/2)):\n                temp_is_sz[temp_annotation.sample[i*2]:temp_annotation.sample[i*2+1]]=1\n        else:\n            to_log = to_log+'No sz.'\n\n        temp_len = temp_edf.n_times\n\n        time_window = 8\n        time_step = 4\n        fs = int(1/(temp_edf.times[1]-temp_edf.times[0]))#T=intervalul de timp dintre 2 esantioane, apoi frecventa f=1/T de esantionare\n        step_window = time_window*fs\n        step = time_step*fs\n\n        temp_is_sz_ind = np.array(\n            [temp_is_sz[i*step:i*step+step_window].sum()/step_window for i in range((temp_len-step_window)//step)]\n        )\n        del temp_is_sz\n\n        temp_0_sample_size = round(p*np.where(temp_is_sz_ind==0)[0].size)\n        temp_1_sample_size = np.where(temp_is_sz_ind>0)[0].size\n\n        # sz data\n        temp_ind = list(np.where(temp_is_sz_ind>0)[0])\n        for i in temp_ind:\n            array_signals[counter, :, :] = temp_signals[:, i*step:i*step+step_window]\n            array_is_sz[counter] = True\n            source_files.append(temp_f)\n            counter = counter+1\n\n        # no sz data\n        temp_ind = random.sample(list(np.where(temp_is_sz_ind==0)[0]), temp_0_sample_size)\n        for i in temp_ind:\n            array_signals[counter, :, :] = temp_signals[:, i*step:i*step+step_window]\n            array_is_sz[counter] = False\n            source_files.append(temp_f)\n            counter = counter+1\n\n        to_log += '{} signals added: {} w/o sz, {} w/ sz.'.format(\n            temp_0_sample_size+temp_1_sample_size, temp_0_sample_size, temp_1_sample_size\n        )\n\n    else:\n        to_log += 'Not appropriate channel labels. Reading skipped.'.format(n)\n    \n    logger.info(to_log)\n    temp_edf.close()\n#Curăță memoria RAM\n    if n%10==0:\n        gc.collect()\ngc.collect()\n# Salvează array-urile rezultate\nnp.save('/kaggle/working/signal_samples.npy', array_signals)\nnp.save('/kaggle/working/is_sz.npy', array_is_sz)\nnp.save('/kaggle/working/source_files.npy', np.array(source_files))\n\narray_signals.shape #(num_windows, num_channels, window_length_samples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:26:15.050699Z","iopub.execute_input":"2025-05-13T19:26:15.051033Z","iopub.status.idle":"2025-05-13T19:37:28.294458Z","shell.execute_reply.started":"2025-05-13T19:26:15.051001Z","shell.execute_reply":"2025-05-13T19:37:28.293605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Încărcarea fișierelor din dataset-ul de pe Kaggle\narray_signals = np.load('/kaggle/input/eeg-seizure-dedtection-rs2023/signal_samples.npy')\narray_is_sz = np.load('/kaggle/input/eeg-seizure-dedtection-rs2023/is_sz.npy')\nsource_files = np.load('/kaggle/input/eeg-seizure-dedtection-rs2023/source_files.npy', allow_pickle=True)\n\n# Verifică dimensiunea pentru a te asigura că sunt încărcate corect\nprint(array_signals.shape)\nprint(array_is_sz.shape)\n#au acelasi nuamr de ferestre deoarece array_is_sz e de tip boolean si apare 0 daca fereastra nu are criza si 1 daca contine\nprint(source_files[:5])  # primele 5 fișiere pentru a verifica\n\n# Checking how much of signals have seizure inside.\n\narray_n = np.where(array_is_sz>.0)[0]\nprint('Number of all the extracted signals: {}'.format(array_is_sz.size))\nprint('Number of signals with seizures: {}'.format(array_n.size))\nprint('Ratio of signals with seizures: {:.3f}'.format(array_n.size/array_is_sz.size))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:10:14.121351Z","iopub.execute_input":"2025-05-19T06:10:14.121654Z","iopub.status.idle":"2025-05-19T06:10:21.222367Z","shell.execute_reply.started":"2025-05-19T06:10:14.121633Z","shell.execute_reply":"2025-05-19T06:10:21.221607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport random\nimport gc\nimport tqdm\nimport logging\nimport mne\nimport wfdb\nimport re\n\n# Creează un sistem de logare pentru monitorizarea fișierelor procesate\nlogger = logging.getLogger(__name__)\nfh = logging.FileHandler('read_files.log')\nlogger.addHandler(fh)\n\n# Parametrii pentru segmentare\ntime_window = 8  # Fereastră de 8 secunde\ntime_step = 4  # Suprapunere de 50% (alunecare de 4 secunde)\np = 0.01  # Proporția de segmente fără crize extrase\ncounter = 0  # Contor pentru numărul total de segmente EEG extrase\n\n# Se citește fiecare fișier EEG\nfor temp_f in files_train:  \n    temp_edf = mne.io.read_raw_edf(temp_f)  \n    temp_labels = temp_edf.ch_names  \n\n    # Verifică dacă toate canalele necesare sunt prezente\n    if sum([any([0 if re.match(c, l) is None else 1 for l in temp_edf.ch_names]) for c in ch_labels]) == len(ch_labels):\n\n        fs = int(1 / (temp_edf.times[1] - temp_edf.times[0]))  # Rata de eșantionare\n        step_window = time_window * fs  # Dimensiunea fiecărei ferestre (în eșantioane)\n        step = time_step * fs  # Alunecare de **4 secunde** pentru suprapunere de 50%\n\n        # Începem segmentarea **de la secunda 1** -> calculăm indexul corespunzător în eșantioane\n        start_index = fs  # 1 sec * frecvența de eșantionare\n\n        temp_is_sz = np.zeros((temp_edf.n_times,))  # Inițializare array cu 0 (fără criză)\n\n        # Verifică dacă fișierul .seizures există și marchează crizele\n        if os.path.exists(temp_f + '.seizures'):\n            temp_annotation = wfdb.rdann(temp_f, 'seizures')\n            for i in range(int(temp_annotation.sample.size / 2)):\n                temp_is_sz[temp_annotation.sample[i * 2]:temp_annotation.sample[i * 2 + 1]] = 1  # Marcare crize\n\n        temp_len = temp_edf.n_times\n\n        # Crearea vectorului de proporție a crizelor **pornind de la secunda 1**\n        temp_is_sz_ind = np.array([\n            temp_is_sz[start_index + i * step:start_index + i * step + step_window].sum() / step_window\n            for i in range((temp_len - start_index - step_window) // step)\n        ])\n\n        # Se calculează câte segmente cu și fără crize vor fi extrase\n        temp_0_sample_size = round(p * np.where(temp_is_sz_ind == 0)[0].size)\n        temp_1_sample_size = np.where(temp_is_sz_ind > 0)[0].size\n\n        counter += temp_0_sample_size + temp_1_sample_size\n\n    temp_edf.close()\n\n# Crearea array-urilor după ce s-au calculat dimensiunile totale\narray_signals = np.zeros((counter, len(ch_labels), step_window), dtype=np.float32)\narray_is_sz = np.zeros(counter, dtype=bool)\nsource_files = []\n\n# Citește din nou fișierele și extrage efectiv semnalele\ncounter = 0\nfor n, temp_f in enumerate(tqdm.tqdm(files_train)):\n    to_log = 'No. {}: Reading. '.format(n)\n    temp_edf = mne.io.read_raw_edf(temp_f)\n\n    temp_labels = temp_edf.ch_names\n    n_label_match = sum([any([0 if re.match(c, l) is None else 1 for l in temp_edf.ch_names]) for c in ch_labels])\n\n    if n_label_match == len(ch_labels):\n        ch_mapping = {sorted([l for l in temp_edf.ch_names if re.match(c, l) is not None])[0]: c for c in ch_labels}\n        temp_edf.rename_channels(ch_mapping)\n\n        temp_is_sz = np.zeros((temp_edf.n_times,))\n        temp_signals = temp_edf.get_data(picks=ch_labels) * 1e6\n\n        if os.path.exists(temp_f + '.seizures'):\n            to_log += 'sz exists.'\n            temp_annotation = wfdb.rdann(temp_f, 'seizures')\n            for i in range(int(temp_annotation.sample.size / 2)):\n                temp_is_sz[temp_annotation.sample[i * 2]:temp_annotation.sample[i * 2 + 1]] = 1\n\n        temp_len = temp_edf.n_times\n\n        fs = int(1 / (temp_edf.times[1] - temp_edf.times[0]))  # Frecvența de eșantionare\n        step_window = time_window * fs\n        step = time_step * fs  # Acum alunecă 4 secunde pentru suprapunere de 50%\n\n        # **Pornim de la secunda 1**\n        start_index = fs  \n\n        temp_is_sz_ind = np.array([\n            temp_is_sz[start_index + i * step:start_index + i * step + step_window].sum() / step_window\n            for i in range((temp_len - start_index - step_window) // step)\n        ])\n        del temp_is_sz\n\n        temp_0_sample_size = round(p * np.where(temp_is_sz_ind == 0)[0].size)\n        temp_1_sample_size = np.where(temp_is_sz_ind > 0)[0].size\n\n        # Adăugarea semnalelor cu crize\n        temp_ind = list(np.where(temp_is_sz_ind > 0)[0])\n        for i in temp_ind:\n            array_signals[counter, :, :] = temp_signals[:, start_index + i * step:start_index + i * step + step_window]\n            array_is_sz[counter] = True\n            source_files.append(temp_f)\n            counter += 1\n\n        # Adăugarea semnalelor fără crize\n        temp_ind = random.sample(list(np.where(temp_is_sz_ind == 0)[0]), temp_0_sample_size)\n        for i in temp_ind:\n            array_signals[counter, :, :] = temp_signals[:, start_index + i * step:start_index + i * step + step_window]\n            array_is_sz[counter] = False\n            source_files.append(temp_f)\n            counter += 1\n\n        to_log += '{} signals added: {} w/o sz, {} w/ sz.'.format(temp_0_sample_size + temp_1_sample_size, temp_0_sample_size, temp_1_sample_size)\n\n    else:\n        to_log += 'Not appropriate channel labels. Reading skipped.'.format(n)\n\n    logger.info(to_log)\n    temp_edf.close()\n\n    if n % 10 == 0:\n        gc.collect()\ngc.collect()\n\n# Salvarea array-urilor rezultate\nnp.save('/kaggle/working/signal_samples_sec1.npy', array_signals)\nnp.save('/kaggle/working/is_sz_sec1.npy', array_is_sz)\nnp.save('/kaggle/working/source_files_sec1.npy', np.array(source_files))\n\narray_signals.shape  # (num_windows, num_channels, window_length_samples)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:14:40.342139Z","iopub.execute_input":"2025-05-20T17:14:40.342813Z","iopub.status.idle":"2025-05-20T17:26:11.345237Z","shell.execute_reply.started":"2025-05-20T17:14:40.342786Z","shell.execute_reply":"2025-05-20T17:26:11.344574Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport random\nimport gc\nimport tqdm\nimport logging\nimport mne\nimport wfdb\nimport re\n\n# Creează un sistem de logare pentru monitorizarea fișierelor procesate\nlogger = logging.getLogger(__name__)\nfh = logging.FileHandler('read_files_sec2.log')\nlogger.addHandler(fh)\n\n# Parametrii pentru segmentare\ntime_window = 8  # Fereastră de 8 secunde\ntime_step = 4  # Suprapunere de 50% (alunecare de 4 secunde)\np = 0.01  # Proporția de segmente fără crize extrase\ncounter = 0  # Contor pentru numărul total de segmente EEG extrase\n\n# Se citește fiecare fișier EEG\nfor temp_f in files_train:  \n    temp_edf = mne.io.read_raw_edf(temp_f, preload=True)  # Optimizare citire\n    temp_labels = temp_edf.ch_names  \n\n    # Verifică dacă toate canalele necesare sunt prezente\n    if sum([any([0 if re.match(c, l) is None else 1 for l in temp_edf.ch_names]) for c in ch_labels]) == len(ch_labels):\n\n        fs = int(1 / (temp_edf.times[1] - temp_edf.times[0]))  # Rata de eșantionare\n        step_window = time_window * fs  # Dimensiunea fiecărei ferestre (în eșantioane)\n        step = time_step * fs  # Alunecare de **4 secunde** pentru suprapunere de 50%\n\n        # Începem segmentarea **de la secunda 2**\n        start_index = 2 * fs  \n\n        temp_is_sz = np.zeros((temp_edf.n_times,))  # Inițializare array cu 0 (fără criză)\n\n        # Verifică dacă fișierul .seizures există și marchează crizele\n        if os.path.exists(temp_f + '.seizures'):\n            temp_annotation = wfdb.rdann(temp_f, 'seizures')\n            for i in range(int(temp_annotation.sample.size / 2)):\n                temp_is_sz[temp_annotation.sample[i * 2]:temp_annotation.sample[i * 2 + 1]] = 1  # Marcare crize\n\n        temp_len = temp_edf.n_times\n\n        # Crearea vectorului de proporție a crizelor **pornind de la secunda 2**\n        temp_is_sz_ind = np.array([\n            temp_is_sz[start_index + i * step:start_index + i * step + step_window].sum() / step_window\n            for i in range((temp_len - start_index - step_window) // step)\n        ])\n\n        # Se calculează câte segmente cu și fără crize vor fi extrase\n        temp_0_sample_size = round(p * np.where(temp_is_sz_ind == 0)[0].size)\n        temp_1_sample_size = np.where(temp_is_sz_ind > 0)[0].size\n\n        counter += temp_0_sample_size + temp_1_sample_size\n\n    temp_edf.close()\n\n# Crearea array-urilor după ce s-au calculat dimensiunile totale\narray_signals = np.zeros((counter, len(ch_labels), step_window), dtype=np.float32)\narray_is_sz = np.zeros(counter, dtype=bool)\nsource_files = []\n\n# Citește din nou fișierele și extrage efectiv semnalele\ncounter = 0\nfor n, temp_f in enumerate(tqdm.tqdm(files_train)):\n    to_log = f\"No. {n}: Reading {temp_f}.\"\n    temp_edf = mne.io.read_raw_edf(temp_f, preload=True)\n\n    temp_labels = temp_edf.ch_names\n    n_label_match = sum([any([0 if re.match(c, l) is None else 1 for l in temp_edf.ch_names]) for c in ch_labels])\n\n    if n_label_match == len(ch_labels):\n        ch_mapping = {sorted([l for l in temp_edf.ch_names if re.match(c, l) is not None])[0]: c for c in ch_labels}\n        temp_edf.rename_channels(ch_mapping)\n\n        temp_is_sz = np.zeros((temp_edf.n_times,))\n        temp_signals = temp_edf.get_data(picks=ch_labels) * 1e6\n\n        if os.path.exists(temp_f + '.seizures'):\n            temp_annotation = wfdb.rdann(temp_f, 'seizures')\n            for i in range(int(temp_annotation.sample.size / 2)):\n                temp_is_sz[temp_annotation.sample[i * 2]:temp_annotation.sample[i * 2 + 1]] = 1\n\n        temp_len = temp_edf.n_times\n\n        fs = int(1 / (temp_edf.times[1] - temp_edf.times[0]))  # Frecvența de eșantionare\n        step_window = time_window * fs\n        step = time_step * fs  # Acum alunecă 4 secunde pentru suprapunere de 50%\n\n        start_index = 2 * fs  \n\n        temp_is_sz_ind = np.array([\n            temp_is_sz[start_index + i * step:start_index + i * step + step_window].sum() / step_window\n            for i in range((temp_len - start_index - step_window) // step)\n        ])\n        del temp_is_sz\n\n        temp_0_sample_size = round(p * np.where(temp_is_sz_ind == 0)[0].size)\n        temp_1_sample_size = np.where(temp_is_sz_ind > 0)[0].size\n\n        # Adăugarea semnalelor cu crize\n        temp_ind = list(np.where(temp_is_sz_ind > 0)[0])\n        for i in temp_ind:\n            if start_index + i * step + step_window <= temp_signals.shape[1]:  # Verificare index!\n                array_signals[counter, :, :] = temp_signals[:, start_index + i * step:start_index + i * step + step_window]\n                array_is_sz[counter] = True\n                source_files.append(temp_f)\n                counter += 1\n            else:\n                print(f\"Skip segment {counter}: Index out of bounds.\")\n\n        # Adăugarea semnalelor fără crize\n        temp_ind = random.sample(list(np.where(temp_is_sz_ind == 0)[0]), temp_0_sample_size)\n        for i in temp_ind:\n            if start_index + i * step + step_window <= temp_signals.shape[1]:  # Verificare index!\n                array_signals[counter, :, :] = temp_signals[:, start_index + i * step:start_index + i * step + step_window]\n                array_is_sz[counter] = False\n                source_files.append(temp_f)\n                counter += 1\n            else:\n                print(f\"Skip segment {counter}: Index out of bounds.\")\n\n        to_log += f\" {temp_0_sample_size + temp_1_sample_size} signals added: {temp_0_sample_size} w/o sz, {temp_1_sample_size} w/ sz.\"\n    else:\n        to_log += \" Not appropriate channel labels. Skipped.\"\n\n    logger.info(to_log)\n    temp_edf.close()\n\n    if n % 10 == 0:\n        gc.collect()\ngc.collect()\n\n# **Salvarea fișierelor**\nnp.save('/kaggle/working/signal_samples_sec2.npy', array_signals)\nnp.save('/kaggle/working/is_sz_sec2.npy', array_is_sz)\nnp.save('/kaggle/working/source_files_sec2.npy', np.array(source_files))\n\narray_signals.shape  # (num_windows, num_channels, window_length_samples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:52:00.868182Z","iopub.execute_input":"2025-05-20T17:52:00.869007Z","iopub.status.idle":"2025-05-20T18:15:23.992845Z","shell.execute_reply.started":"2025-05-20T17:52:00.868976Z","shell.execute_reply":"2025-05-20T18:15:23.991921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport random\nimport gc\nimport tqdm\nimport logging\nimport mne\nimport wfdb\nimport re\n\n# Setare sistem de logare\nlogger = logging.getLogger(__name__)\nfh = logging.FileHandler('read_files_sec3.log')\nlogger.addHandler(fh)\n\n# Parametrii pentru segmentare\ntime_window = 8  \ntime_step = 4  \np = 0.01  \ncounter = 0  \n\n# Citirea fișierelor EEG\nfor temp_f in files_train:  \n    temp_edf = mne.io.read_raw_edf(temp_f, preload=True)  \n    temp_labels = temp_edf.ch_names  \n\n    if sum([any([0 if re.match(c, l) is None else 1 for l in temp_edf.ch_names]) for c in ch_labels]) == len(ch_labels):\n\n        fs = int(1 / (temp_edf.times[1] - temp_edf.times[0]))  \n        step_window = time_window * fs  \n        step = time_step * fs  \n\n        start_index = 3 * fs  \n\n        temp_is_sz = np.zeros((temp_edf.n_times,))  \n\n        if os.path.exists(temp_f + '.seizures'):\n            temp_annotation = wfdb.rdann(temp_f, 'seizures')\n            for i in range(int(temp_annotation.sample.size / 2)):\n                temp_is_sz[temp_annotation.sample[i * 2]:temp_annotation.sample[i * 2 + 1]] = 1  \n\n        temp_len = temp_edf.n_times\n\n        temp_is_sz_ind = np.array([\n            temp_is_sz[start_index + i * step:start_index + i * step + step_window].sum() / step_window\n            for i in range((temp_len - start_index - step_window) // step)\n        ])\n\n        temp_0_sample_size = round(p * np.where(temp_is_sz_ind == 0)[0].size)\n        temp_1_sample_size = np.where(temp_is_sz_ind > 0)[0].size\n\n        counter += temp_0_sample_size + temp_1_sample_size\n\n    temp_edf.close()\n\n# Crearea array-urilor după calcul\narray_signals = np.zeros((counter, len(ch_labels), step_window), dtype=np.float32)\narray_is_sz = np.zeros(counter, dtype=bool)\nsource_files = []\n\n# Citirea fișierelor și extragerea semnalelor\ncounter = 0\nfor n, temp_f in enumerate(tqdm.tqdm(files_train)):\n    to_log = f\"No. {n}: Reading {temp_f}.\"\n    temp_edf = mne.io.read_raw_edf(temp_f, preload=True)\n\n    temp_labels = temp_edf.ch_names\n    n_label_match = sum([any([0 if re.match(c, l) is None else 1 for l in temp_edf.ch_names]) for c in ch_labels])\n\n    if n_label_match == len(ch_labels):\n        ch_mapping = {sorted([l for l in temp_edf.ch_names if re.match(c, l) is not None])[0]: c for c in ch_labels}\n        temp_edf.rename_channels(ch_mapping)\n\n        temp_is_sz = np.zeros((temp_edf.n_times,))\n        temp_signals = temp_edf.get_data(picks=ch_labels) * 1e6\n\n        if os.path.exists(temp_f + '.seizures'):\n            temp_annotation = wfdb.rdann(temp_f, 'seizures')\n            for i in range(int(temp_annotation.sample.size / 2)):\n                temp_is_sz[temp_annotation.sample[i * 2]:temp_annotation.sample[i * 2 + 1]] = 1\n\n        temp_len = temp_edf.n_times\n\n        fs = int(1 / (temp_edf.times[1] - temp_edf.times[0]))  \n        step_window = time_window * fs\n        step = time_step * fs  \n\n        start_index = 3 * fs  \n\n        temp_is_sz_ind = np.array([\n            temp_is_sz[start_index + i * step:start_index + i * step + step_window].sum() / step_window\n            for i in range((temp_len - start_index - step_window) // step)\n        ])\n        del temp_is_sz\n\n        temp_0_sample_size = round(p * np.where(temp_is_sz_ind == 0)[0].size)\n        temp_1_sample_size = np.where(temp_is_sz_ind > 0)[0].size\n\n        temp_ind = list(np.where(temp_is_sz_ind > 0)[0])\n        for i in temp_ind:\n            if start_index + i * step + step_window <= temp_signals.shape[1]:  \n                array_signals[counter, :, :] = temp_signals[:, start_index + i * step:start_index + i * step + step_window]\n                array_is_sz[counter] = True\n                source_files.append(temp_f)\n                counter += 1\n\n        temp_ind = random.sample(list(np.where(temp_is_sz_ind == 0)[0]), temp_0_sample_size)\n        for i in temp_ind:\n            if start_index + i * step + step_window <= temp_signals.shape[1]:  \n                array_signals[counter, :, :] = temp_signals[:, start_index + i * step:start_index + i * step + step_window]\n                array_is_sz[counter] = False\n                source_files.append(temp_f)\n                counter += 1\n\n        to_log += f\" {temp_0_sample_size + temp_1_sample_size} signals added: {temp_0_sample_size} w/o sz, {temp_1_sample_size} w/ sz.\"\n    else:\n        to_log += \" Not appropriate channel labels. Skipped.\"\n\n    logger.info(to_log)\n    temp_edf.close()\n\n    if n % 10 == 0:\n        gc.collect()\ngc.collect()\n\n# Salvarea fișierelor\nnp.save('/kaggle/working/signal_samples_sec3.npy', array_signals)\nnp.save('/kaggle/working/is_sz_sec3.npy', array_is_sz)\nnp.save('/kaggle/working/source_files_sec3.npy', np.array(source_files))\n\narray_signals.shape  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T18:23:08.249199Z","iopub.execute_input":"2025-05-20T18:23:08.249796Z","iopub.status.idle":"2025-05-20T18:46:23.798744Z","shell.execute_reply.started":"2025-05-20T18:23:08.249773Z","shell.execute_reply":"2025-05-20T18:46:23.798084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import os\n# import random\n# import gc\n# import tqdm\n# import logging\n# import mne\n# import wfdb\n# import re\n\n# # Creează un sistem de logare pentru monitorizarea fișierelor procesate\n# logger = logging.getLogger(__name__)\n# fh = logging.FileHandler('read_files.log')\n# logger.addHandler(fh)\n\n# # Parametrii pentru segmentare\n# time_window = 8  # Fereastră de 8 secunde\n# time_step = 4  # Suprapunere de 50% (alunecare de 4 secunde)\n# p = 0.01  # Proporția de segmente fără crize extrase\n# counter = 0  # Contor pentru numărul total de segmente EEG extrase\n\n# # Se citește fiecare fișier EEG\n# for temp_f in files_train:  \n#     temp_edf = mne.io.read_raw_edf(temp_f)  \n#     temp_labels = temp_edf.ch_names  \n\n#     # Verifică dacă toate canalele necesare sunt prezente\n#     if sum([any([0 if re.match(c, l) is None else 1 for l in temp_edf.ch_names]) for c in ch_labels]) == len(ch_labels):\n\n#         fs = int(1 / (temp_edf.times[1] - temp_edf.times[0]))  # Rata de eșantionare\n#         step_window = time_window * fs  # Dimensiunea fiecărei ferestre (în eșantioane)\n#         step = time_step * fs  # Alunecare de **4 secunde** pentru suprapunere de 50%\n\n#         # Începem segmentarea **de la secunda 1** -> calculăm indexul corespunzător în eșantioane\n#         start_index = fs  # 1 sec * frecvența de eșantionare\n\n#         temp_is_sz = np.zeros((temp_edf.n_times,))  # Inițializare array cu 0 (fără criză)\n\n#         # Verifică dacă fișierul .seizures există și marchează crizele\n#         if os.path.exists(temp_f + '.seizures'):\n#             temp_annotation = wfdb.rdann(temp_f, 'seizures')\n#             for i in range(int(temp_annotation.sample.size / 2)):\n#                 temp_is_sz[temp_annotation.sample[i * 2]:temp_annotation.sample[i * 2 + 1]] = 1  # Marcare crize\n\n#         temp_len = temp_edf.n_times\n\n#         # Crearea vectorului de proporție a crizelor **pornind de la secunda 1**\n#         temp_is_sz_ind = np.array([\n#             temp_is_sz[start_index + i * step:start_index + i * step + step_window].sum() / step_window\n#             for i in range((temp_len - start_index - step_window) // step)\n#         ])\n\n#         # Se calculează câte segmente cu și fără crize vor fi extrase\n#         temp_0_sample_size = round(p * np.where(temp_is_sz_ind == 0)[0].size)\n#         temp_1_sample_size = np.where(temp_is_sz_ind > 0)[0].size\n\n#         counter += temp_0_sample_size + temp_1_sample_size\n\n#     temp_edf.close()\n\n# # Crearea array-urilor după ce s-au calculat dimensiunile totale\n# array_signals = np.zeros((counter, len(ch_labels), step_window), dtype=np.float32)\n# array_is_sz = np.zeros(counter, dtype=bool)\n# source_files = []\n\n# # Citește din nou fișierele și extrage efectiv semnalele\n# counter = 0\n# for n, temp_f in enumerate(tqdm.tqdm(files_train)):\n#     to_log = 'No. {}: Reading. '.format(n)\n#     temp_edf = mne.io.read_raw_edf(temp_f)\n\n#     temp_labels = temp_edf.ch_names\n#     n_label_match = sum([any([0 if re.match(c, l) is None else 1 for l in temp_edf.ch_names]) for c in ch_labels])\n\n#     if n_label_match == len(ch_labels):\n#         ch_mapping = {sorted([l for l in temp_edf.ch_names if re.match(c, l) is not None])[0]: c for c in ch_labels}\n#         temp_edf.rename_channels(ch_mapping)\n\n#         temp_is_sz = np.zeros((temp_edf.n_times,))\n#         temp_signals = temp_edf.get_data(picks=ch_labels) * 1e6\n\n#         if os.path.exists(temp_f + '.seizures'):\n#             to_log += 'sz exists.'\n#             temp_annotation = wfdb.rdann(temp_f, 'seizures')\n#             for i in range(int(temp_annotation.sample.size / 2)):\n#                 temp_is_sz[temp_annotation.sample[i * 2]:temp_annotation.sample[i * 2 + 1]] = 1\n\n#         temp_len = temp_edf.n_times\n\n#         fs = int(1 / (temp_edf.times[1] - temp_edf.times[0]))  # Frecvența de eșantionare\n#         step_window = time_window * fs\n#         step = time_step * fs  # Acum alunecă 4 secunde pentru suprapunere de 50%\n\n#         # **Pornim de la secunda 2**\n#         start_index = 2*fs  \n\n#         temp_is_sz_ind = np.array([\n#             temp_is_sz[start_index + i * step:start_index + i * step + step_window].sum() / step_window\n#             for i in range((temp_len - start_index - step_window) // step)\n#         ])\n#         del temp_is_sz\n\n#         temp_0_sample_size = round(p * np.where(temp_is_sz_ind == 0)[0].size)\n#         temp_1_sample_size = np.where(temp_is_sz_ind > 0)[0].size\n\n#         # Adăugarea semnalelor cu crize\n#         temp_ind = list(np.where(temp_is_sz_ind > 0)[0])\n#         for i in temp_ind:\n#             array_signals[counter, :, :] = temp_signals[:, start_index + i * step:start_index + i * step + step_window]\n#             array_is_sz[counter] = True\n#             source_files.append(temp_f)\n#             counter += 1\n\n#         # Adăugarea semnalelor fără crize\n#         temp_ind = random.sample(list(np.where(temp_is_sz_ind == 0)[0]), temp_0_sample_size)\n#         for i in temp_ind:\n#             array_signals[counter, :, :] = temp_signals[:, start_index + i * step:start_index + i * step + step_window]\n#             array_is_sz[counter] = False\n#             source_files.append(temp_f)\n#             counter += 1\n\n#         to_log += '{} signals added: {} w/o sz, {} w/ sz.'.format(temp_0_sample_size + temp_1_sample_size, temp_0_sample_size, temp_1_sample_size)\n\n#     else:\n#         to_log += 'Not appropriate channel labels. Reading skipped.'.format(n)\n\n#     logger.info(to_log)\n#     temp_edf.close()\n\n#     if n % 10 == 0:\n#         gc.collect()\n# gc.collect()\n\n# # Salvarea array-urilor rezultate\n# np.save('/kaggle/working/signal_samples.npy', array_signals)\n# np.save('/kaggle/working/is_sz.npy', array_is_sz)\n# np.save('/kaggle/working/source_files.npy', np.array(source_files))\n\n# array_signals.shape  # (num_windows, num_channels, window_length_samples)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T17:33:59.660455Z","iopub.execute_input":"2025-05-20T17:33:59.661071Z","iopub.status.idle":"2025-05-20T17:44:46.950518Z","shell.execute_reply.started":"2025-05-20T17:33:59.661037Z","shell.execute_reply":"2025-05-20T17:44:46.949698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #incepand cu secundele 1,2,3 pentru toti copiii\n# !pip install wfdb\n\n# import logging\n# import random\n# import numpy as np\n# import mne\n# import wfdb\n# import os\n# import gc\n# import tqdm\n# import matplotlib.pyplot as plt\n\n# # Set up logging\n# logger = logging.getLogger(__name__)\n# fh = logging.FileHandler('read_files.log')\n# logger.addHandler(fh)\n\n# # Window parameters\n# time_window = 8  # 8 seconds\n# fs = 256  # default sampling frequency\n# step_window = time_window * fs\n# p = 0.01\n\n# # Calculate total segments across all files\n# counter = 0\n# for temp_f in files_train:\n#     temp_edf = mne.io.read_raw_edf(temp_f)\n#     if all(any(re.match(c, l) for l in temp_edf.ch_names) for c in ch_labels):\n#         temp_len = temp_edf.n_times\n#         temp_is_sz = np.zeros(temp_len)\n#         if os.path.exists(temp_f + '.seizures'):\n#             temp_annotation = wfdb.rdann(temp_f, 'seizures')\n#             for i in range(temp_annotation.sample.size // 2):\n#                 temp_is_sz[temp_annotation.sample[i * 2] : temp_annotation.sample[i * 2 + 1]] = 1\n        \n#         # Include start offsets (0s, 1s, 2s, 3s)\n#         for offset in range(fs, 4 * fs, fs):\n#             temp_is_sz_ind = np.array([\n#                 temp_is_sz[i * fs + offset : i * fs + offset + step_window].sum() / step_window\n#                 for i in range((temp_len - offset - step_window) // fs)\n#             ])\n#             counter += round(p * np.where(temp_is_sz_ind == 0)[0].size)\n#             counter += np.where(temp_is_sz_ind > 0)[0].size\n#     temp_edf.close()\n\n# del temp_is_sz\n\n# # Initialize arrays\n# array_signals = np.zeros((counter, len(ch_labels), step_window), dtype=np.float32)\n# array_is_sz = np.zeros(counter, dtype=bool)\n# source_files = []\n\n# # Extract and store segments\n# counter = 0\n# for n, temp_f in enumerate(tqdm.tqdm(files_train)):\n#     temp_edf = mne.io.read_raw_edf(temp_f)\n#     if all(any(re.match(c, l) for l in temp_edf.ch_names) for c in ch_labels):\n#         temp_signals = temp_edf.get_data(picks=ch_labels) * 1e6\n#         temp_len = temp_edf.n_times\n#         temp_is_sz = np.zeros(temp_len)\n#         if os.path.exists(temp_f + '.seizures'):\n#             temp_annotation = wfdb.rdann(temp_f, 'seizures')\n#             for i in range(temp_annotation.sample.size // 2):\n#                 temp_is_sz[temp_annotation.sample[i * 2] : temp_annotation.sample[i * 2 + 1]] = 1\n        \n#         for offset in range(0, 4 * fs, fs):\n#             temp_is_sz_ind = np.array([\n#                 temp_is_sz[i * fs + offset : i * fs + offset + step_window].sum() / step_window\n#                 for i in range((temp_len - offset - step_window) // fs)\n#             ])\n            \n#             # Extract seizure data\n#             temp_ind = list(np.where(temp_is_sz_ind > 0)[0])\n#             for i in temp_ind:\n#                 array_signals[counter, :, :] = temp_signals[:, i * fs + offset : i * fs + offset + step_window]\n#                 array_is_sz[counter] = True\n#                 source_files.append(temp_f)\n#                 counter += 1\n            \n#             # Extract non-seizure data\n#             temp_0_sample_size = round(p * np.where(temp_is_sz_ind == 0)[0].size)\n#             temp_ind = random.sample(list(np.where(temp_is_sz_ind == 0)[0]), temp_0_sample_size)\n#             for i in temp_ind:\n#                 array_signals[counter, :, :] = temp_signals[:, i * fs + offset : i * fs + offset + step_window]\n#                 array_is_sz[counter] = False\n#                 source_files.append(temp_f)\n#                 counter += 1\n#     temp_edf.close()\n#     if n % 10 == 0:\n#         gc.collect()\n\n# gc.collect()\n# np.save('/kaggle/working/signal_samples_2.npy', array_signals)\n# np.save('/kaggle/working/is_sz_2.npy', array_is_sz)\n# np.save('/kaggle/working/source_files.npy', np.array(source_files))\n\n# # Visualization of window distribution\n# signals = np.load('/kaggle/working/signal_samples_2.npy')\n# labels = np.load('/kaggle/working/is_sz_2.npy')\n# sources = np.load('/kaggle/working/source_files_2.npy')\n\n# print(\"Total number of windows:\", signals.shape[0])\n# print(\"Number of seizure windows:\", labels.sum())\n# print(\"Number of non-seizure windows:\", len(labels) - labels.sum())\n\n# plt.figure(figsize=(10, 6))\n# plt.hist(labels, bins=2, color='skyblue', rwidth=0.8)\n# plt.xticks([0, 1], [\"Non-Seizure\", \"Seizure\"])\n# plt.title(\"Distribution of Seizure and Non-Seizure Windows\")\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import mne\n\n# # Verifică canalele disponibile în primul fișier chb02\n# temp_f = [f for f in files_train if \"chb02_\" in f][0]\n# raw = mne.io.read_raw_edf(temp_f, preload=False)\n# print(raw.ch_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T10:03:24.418735Z","iopub.execute_input":"2025-05-14T10:03:24.419077Z","iopub.status.idle":"2025-05-14T10:03:25.200105Z","shell.execute_reply.started":"2025-05-14T10:03:24.419054Z","shell.execute_reply":"2025-05-14T10:03:25.198795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #incepand cu secundele 1,2,3 pentru chb_02\n# import logging\n# import random\n# import numpy as np\n# import mne\n# import wfdb\n# import os\n# import gc\n# import tqdm\n# import matplotlib.pyplot as plt\n\n# # Set up logging\n# logger = logging.getLogger(__name__)\n# fh = logging.FileHandler('read_files_chb02.log')\n# logger.addHandler(fh)\n\n# # Window parameters\n# time_window = 8  # 8 seconds\n# fs = 256  # default sampling frequency\n# step_window = time_window * fs\n# p = 0.01\n\n# ch_labels = ['FP1-F7', 'F7-T7', 'T7-P7', 'P7-O1', 'FP1-F3', 'F3-C3', 'C3-P3', \n#              'P3-O1', 'FP2-F4', 'F4-C4', 'C4-P4', 'P4-O2', 'FP2-F8', 'F8-T8', \n#              'T8-P8-0', 'P8-O2', 'FZ-CZ', 'CZ-PZ', 'P7-T7', 'T7-FT9', 'FT9-FT10', \n#              'FT10-T8', 'T8-P8-1']\n\n# # Filter files for chb02 only\n# files_chb02 = [f for f in files_train if \"chb02_\" in f]\n\n# # Calculate total segments for chb02\n# counter = 0\n# for temp_f in files_chb02:\n#     temp_edf = mne.io.read_raw_edf(temp_f)\n#     if all(any(re.match(c, l) for l in temp_edf.ch_names) for c in ch_labels):\n#         temp_len = temp_edf.n_times\n#         temp_is_sz = np.zeros(temp_len)\n#         if os.path.exists(temp_f + '.seizures'):\n#             temp_annotation = wfdb.rdann(temp_f, 'seizures')\n#             for i in range(temp_annotation.sample.size // 2):\n#                 temp_is_sz[temp_annotation.sample[i * 2] : temp_annotation.sample[i * 2 + 1]] = 1\n        \n#         # Include start offsets (1s, 2s, 3s)\n#         for offset in [1 * fs, 2 * fs, 3 * fs]:\n#             temp_is_sz_ind = np.array([\n#                 temp_is_sz[i * fs + offset : i * fs + offset + step_window].sum() / step_window\n#                 for i in range((temp_len - offset - step_window) // fs)\n#             ])\n#             counter += round(p * np.where(temp_is_sz_ind == 0)[0].size)\n#             counter += np.where(temp_is_sz_ind > 0)[0].size\n#     temp_edf.close()\n\n# del temp_is_sz\n\n# # Initialize arrays\n# array_signals_02 = np.zeros((counter, len(ch_labels), step_window), dtype=np.float32)\n# array_is_sz_02 = np.zeros(counter, dtype=bool)\n# source_files = []\n\n# # Extract and store segments for chb02\n# counter = 0\n# for n, temp_f in enumerate(tqdm.tqdm(files_chb02)):\n#     temp_edf = mne.io.read_raw_edf(temp_f)\n#     if all(any(re.match(c, l) for l in temp_edf.ch_names) for c in ch_labels):\n#         temp_signals = temp_edf.get_data(picks=ch_labels) * 1e6\n#         temp_len = temp_edf.n_times\n#         temp_is_sz = np.zeros(temp_len)\n#         if os.path.exists(temp_f + '.seizures'):\n#             temp_annotation = wfdb.rdann(temp_f, 'seizures')\n#             for i in range(temp_annotation.sample.size // 2):\n#                 temp_is_sz[temp_annotation.sample[i * 2] : temp_annotation.sample[i * 2 + 1]] = 1\n        \n#         # Include start offsets (1s, 2s, 3s)\n#         for offset in [1 * fs, 2 * fs, 3 * fs]:\n#             temp_is_sz_ind = np.array([\n#                 temp_is_sz[i * fs + offset : i * fs + offset + step_window].sum() / step_window\n#                 for i in range((temp_len - offset - step_window) // fs)\n#             ])\n            \n#             # Extract seizure data\n#             temp_ind = list(np.where(temp_is_sz_ind > 0)[0])\n#             for i in temp_ind:\n#                 array_signals_02[counter, :, :] = temp_signals[:, i * fs + offset : i * fs + offset + step_window]\n#                 array_is_sz_02[counter] = True\n#                 source_files.append(temp_f)\n#                 counter += 1\n            \n#             # Extract non-seizure data\n#             temp_0_sample_size = round(p * np.where(temp_is_sz_ind == 0)[0].size)\n#             temp_ind = random.sample(list(np.where(temp_is_sz_ind == 0)[0]), temp_0_sample_size)\n#             for i in temp_ind:\n#                 array_signals_02[counter, :, :] = temp_signals[:, i * fs + offset : i * fs + offset + step_window]\n#                 array_is_sz_02[counter] = False\n#                 source_files.append(temp_f)\n#                 counter += 1\n#     temp_edf.close()\n#     if n % 10 == 0:\n#         gc.collect()\n\n# gc.collect()\n# np.save('/kaggle/working/signal_samples_chb02.npy', array_signals)\n# np.save('/kaggle/working/is_sz_chb02.npy', array_is_sz)\n# np.save('/kaggle/working/source_files_chb02.npy', np.array(source_files))\n\n# # Visualization of window distribution\n# signals = np.load('/kaggle/working/signal_samples_chb02.npy')\n# labels = np.load('/kaggle/working/is_sz_chb02.npy')\n# sources = np.load('/kaggle/working/source_files_chb02.npy')\n\n# print(\"Total number of windows (chb02):\", signals.shape[0])\n# print(\"Number of seizure windows (chb02):\", labels.sum())\n# print(\"Number of non-seizure windows (chb02):\", len(labels) - labels.sum())\n\n# plt.figure(figsize=(10, 6))\n# plt.hist(labels, bins=2, color='skyblue', rwidth=0.8)\n# plt.xticks([0, 1], [\"Non-Seizure\", \"Seizure\"])\n# plt.title(\"Distribution of Seizure and Non-Seizure Windows (chb02)\")\n# plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T19:59:24.380934Z","iopub.execute_input":"2025-05-13T19:59:24.381669Z","iopub.status.idle":"2025-05-13T19:59:58.137441Z","shell.execute_reply.started":"2025-05-13T19:59:24.381645Z","shell.execute_reply":"2025-05-13T19:59:58.136637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#random seed 2023\nimport numpy as np\n\narray_signals = np.load('/kaggle/input/eeg-seizure-dedtection-rs2023/signal_samples.npy')\narray_is_sz = np.load('/kaggle/input/eeg-seizure-dedtection-rs2023/is_sz.npy')\nsource_files = np.load('/kaggle/input/eeg-seizure-dedtection-rs2023/source_files.npy', allow_pickle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:10:33.329127Z","iopub.execute_input":"2025-05-19T06:10:33.329441Z","iopub.status.idle":"2025-05-19T06:10:33.877835Z","shell.execute_reply.started":"2025-05-19T06:10:33.329393Z","shell.execute_reply":"2025-05-19T06:10:33.877271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"array_signals.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:10:36.282563Z","iopub.execute_input":"2025-05-19T06:10:36.282822Z","iopub.status.idle":"2025-05-19T06:10:36.288125Z","shell.execute_reply.started":"2025-05-19T06:10:36.282803Z","shell.execute_reply":"2025-05-19T06:10:36.287474Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"array_is_sz.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:10:44.883127Z","iopub.execute_input":"2025-05-19T06:10:44.883609Z","iopub.status.idle":"2025-05-19T06:10:44.888675Z","shell.execute_reply.started":"2025-05-19T06:10:44.883583Z","shell.execute_reply":"2025-05-19T06:10:44.887647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"source_files.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:10:46.793747Z","iopub.execute_input":"2025-05-19T06:10:46.794430Z","iopub.status.idle":"2025-05-19T06:10:46.798767Z","shell.execute_reply.started":"2025-05-19T06:10:46.794384Z","shell.execute_reply":"2025-05-19T06:10:46.798198Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Secțiunea 3.2 - Graphics: Samples of extracted signals","metadata":{}},{"cell_type":"code","source":"# show a sample of extracted signals, one plot, the first two and the last two\n\nimport matplotlib.pyplot as plt\nvertical_width = 250\n#----------------------------------------------------------\n#frecventa 2048\nsignals = array_signals[1, :, :]\nfs = 128 #the frequency was resampled\n\nfig, ax = plt.subplots()\nfor i in range(signals.shape[0]):\n    ax.plot(np.arange(signals.shape[-1])/fs, signals[i, :]+i*vertical_width, linewidth=0.5, color='tab:blue')\n    ax.annotate(ch_labels[i], xy=(0, i*vertical_width))\nax.invert_yaxis()\nplt.show()\n#---------------------------frecventa 1024\nsignals = array_signals[1, :, ::2]\nfs = 128 #the frequency was resampled\n\nfig, ax = plt.subplots()\nfor i in range(signals.shape[0]):\n    ax.plot(np.arange(signals.shape[-1])/fs, signals[i, :]+i*vertical_width, linewidth=0.5, color='tab:blue')\n    ax.annotate(ch_labels[i], xy=(0, i*vertical_width))\nax.invert_yaxis()\nplt.show()\n#-----------------------------primele 3 canale\nsignals = array_signals[1, :3, ::2]\nfs = 128 #the frequency was resampled\n\nfig, ax = plt.subplots()\nfor i in range(signals.shape[0]):\n    ax.plot(np.arange(signals.shape[-1])/fs, signals[i, :]+i*vertical_width, linewidth=0.5, color='tab:blue')\n    ax.annotate(ch_labels[i], xy=(0, i*vertical_width))\nax.invert_yaxis()\nplt.show()\n#-------------------------------------------------------\nsignals = array_signals[2, :, ::2]\nfs = 128 #the frequency was resampled\n\nfig, ax = plt.subplots(figsize=(15, 6))\nfor i in range(signals.shape[0]):\n    ax.plot(np.arange(signals.shape[-1])/fs, signals[i, :]+i*vertical_width, linewidth=0.5, color='tab:blue')\n    ax.annotate(ch_labels[i], xy=(0, i*vertical_width))\nax.invert_yaxis()\nplt.show()\n#-------------------------------------------------------\nsignals = array_signals[-2, :, ::2]\nfs = 128 #the frequency was resampled\n\nfig, ax = plt.subplots(figsize=(10, 8))\nfor i in range(signals.shape[0]):\n    ax.plot(np.arange(signals.shape[-1])/fs, signals[i, :]+i*vertical_width, linewidth=0.5, color='tab:blue')\n    ax.annotate(ch_labels[i], xy=(0, i*vertical_width))\nax.invert_yaxis()\nplt.show()\n#----------------------------------------------------------\nsignals = array_signals[-1, :, ::2]\nfs = 128 #the frequency was resampled\n\nfig, ax = plt.subplots(figsize=(10, 6))\nfor i in range(signals.shape[0]):\n    ax.plot(np.arange(signals.shape[-1])/fs, signals[i, :]+i*vertical_width, linewidth=0.5, color='tab:blue')\n    ax.annotate(ch_labels[i], xy=(0, i*vertical_width))\nax.invert_yaxis()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:10:50.583543Z","iopub.execute_input":"2025-05-19T06:10:50.583805Z","iopub.status.idle":"2025-05-19T06:10:51.864838Z","shell.execute_reply.started":"2025-05-19T06:10:50.583785Z","shell.execute_reply":"2025-05-19T06:10:51.864116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Samples with seizures. Two plots, with heatmaps\nimport random\nimport os\n\nfor n in random.sample(list(array_n), 10):\n    temp_signals = array_signals[n, :, :]\n    fs = 128\n    vertical_width = 300\n    file_origin = source_files[n]\n    file_short = os.path.basename(file_origin)\n\n    #creeaza grafic cu 2 sub-grafice\n    fig, ax = plt.subplots(2, 1, figsize=(10, 6), gridspec_kw={'height_ratios': [3, 1]})\n    \n    # Subgraficul 0 - semnal EEG (multi-canal)\n    for i in range(temp_signals.shape[0]):\n        ax[0].plot(np.arange(temp_signals.shape[-1]) / fs,\n                   temp_signals[i, :] + i * vertical_width,\n                   linewidth=0.5, color='tab:blue')\n        ax[0].annotate(ch_labels[i], xy=(0, i * vertical_width))\n    \n    ax[0].invert_yaxis()\n    ax[0].set_xlim(0, 8)\n    ax[0].set_title(f'Sample no. {n} | Source: {file_short}')\n\n    # Subgraficul 1 - heatmap\n    ax[1].pcolormesh(np.arange(temp_signals.shape[-1]) / fs,\n                     np.arange(len(ch_labels)),\n                     temp_signals[:, :], cmap='gray')\n    ax[1].invert_yaxis()\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:10:57.041625Z","iopub.execute_input":"2025-05-19T06:10:57.042169Z","iopub.status.idle":"2025-05-19T06:11:01.153016Z","shell.execute_reply.started":"2025-05-19T06:10:57.042145Z","shell.execute_reply":"2025-05-19T06:11:01.152213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#random seed 17\nimport numpy as np\n\narray_signals = np.load('/kaggle/input/eeg-processed-samples/signal_samples.npy')\narray_is_sz = np.load('/kaggle/input/eeg-processed-samples/is_sz.npy')\nsource_files = np.load('/kaggle/input/eeg-processed-samples/source_files.npy', allow_pickle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T11:43:12.861829Z","iopub.execute_input":"2025-05-16T11:43:12.862159Z","iopub.status.idle":"2025-05-16T11:43:21.539731Z","shell.execute_reply.started":"2025-05-16T11:43:12.862106Z","shell.execute_reply":"2025-05-16T11:43:21.538939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#random seed 2023\nimport numpy as np\n\narray_signals = np.load('/kaggle/input/eeg-seizure-dedtection-rs2023/signal_samples.npy')\narray_is_sz = np.load('/kaggle/input/eeg-seizure-dedtection-rs2023/is_sz.npy')\nsource_files = np.load('/kaggle/input/eeg-seizure-dedtection-rs2023/source_files.npy', allow_pickle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T06:42:37.058348Z","iopub.execute_input":"2025-05-14T06:42:37.058594Z","iopub.status.idle":"2025-05-14T06:42:45.168849Z","shell.execute_reply.started":"2025-05-14T06:42:37.058570Z","shell.execute_reply":"2025-05-14T06:42:45.168218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3. Channel dimension and data split","metadata":{}},{"cell_type":"code","source":"array_signals = array_signals[:, :, ::2]\narray_signals.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:11:06.897190Z","iopub.execute_input":"2025-05-19T06:11:06.897732Z","iopub.status.idle":"2025-05-19T06:11:06.902577Z","shell.execute_reply.started":"2025-05-19T06:11:06.897711Z","shell.execute_reply":"2025-05-19T06:11:06.901895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CNN will be used. Channel dimension is added.\n\narray_signals = array_signals[:, :, :, np.newaxis]\n\narray_signals.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:11:09.697774Z","iopub.execute_input":"2025-05-19T06:11:09.698338Z","iopub.status.idle":"2025-05-19T06:11:09.702763Z","shell.execute_reply.started":"2025-05-19T06:11:09.698315Z","shell.execute_reply":"2025-05-19T06:11:09.702138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Split data\nimport numpy as np\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\n\n# Împărțim datele: 80% pentru antrenare, 20% pentru testare\nX_train, X_test, y_train, y_test = train_test_split(\n    array_signals,        # semnalele EEG\n    array_is_sz,          # etichetele (True = criză, False = non-criză)\n    test_size=0.3,        # 30% din date merg în setul de test\n    random_state=42,      # pentru reproductibilitate\n    stratify=array_is_sz  # păstrează proporția dintre clase (criză/non-criză)\n)\nprint(\"Dimensiuni X_train:\", X_train.shape)\nprint(\"Dimensiuni y_train:\", y_train.shape)\nprint(\"Dimensiuni X_test:\", X_test.shape)\nprint(\"Dimensiuni y_test:\", y_test.shape)\n\n# # Adăugăm expand_dim pentru CNN 2D\n# X_train = np.expand_dims(X_train, axis=-1)  # (8038, 18, 2048, 1)\n# X_test = np.expand_dims(X_test, axis=-1)    # (2010, 18, 2048, 1)\n\n# # Normalizare între 0 și 1\n# X_train = (X_train - np.min(X_train)) / (np.max(X_train) - np.min(X_train))\n# X_test = (X_test - np.min(X_test)) / (np.max(X_test) - np.min(X_test))\n\n# print(\"Dimensiuni actualizate X_train:\", X_train.shape)\n# print(\"Dimensiuni actualizate y_train:\", y_train.shape)\n# print(\"Dimensiuni actualizate X_test:\", X_test.shape)\n# print(\"Dimensiuni actualizate y_test:\", y_test.shape)\n\n# Funcție pentru afișarea distribuției etichetelor\ndef show_distribution(y, name):\n    unique, counts = np.unique(y, return_counts=True)\n    total = counts.sum()\n    print(f\"\\nDistribuție {name}:\")\n    for val, cnt in zip(unique, counts):\n        pct = 100 * cnt / total\n        label = \"criză\" if val == 1 else \"non-criză\"\n        print(f\"  {label} ({val}): {cnt} ({pct:.2f}%)\")\n\n# Afișarea distribuției\nshow_distribution(y_train, \"y_train\")\nshow_distribution(y_test, \"y_test\")\n\nfrom sklearn.utils.class_weight import compute_class_weight\n\n\n# Calculăm ponderile pentru clase\nclasses = np.unique(y_train)\nclass_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\nclass_weights = {i: w for i, w in zip(classes, class_weights)}\n\nprint(\"\\nPonderi clase:\", class_weights)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:11:47.906846Z","iopub.execute_input":"2025-05-19T06:11:47.907123Z","iopub.status.idle":"2025-05-19T06:11:48.190572Z","shell.execute_reply.started":"2025-05-19T06:11:47.907103Z","shell.execute_reply":"2025-05-19T06:11:48.189913Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Secțiunea 4 - Antrenarea modelului de învațare automata utilizând CNN 2D","metadata":{}},{"cell_type":"code","source":"#verifies if there is a GPU\nimport tensorflow as tf\nprint(\"GPU available:\", tf.config.list_physical_devices('GPU'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:12:13.721368Z","iopub.execute_input":"2025-05-19T06:12:13.721685Z","iopub.status.idle":"2025-05-19T06:12:27.038232Z","shell.execute_reply.started":"2025-05-19T06:12:13.721666Z","shell.execute_reply":"2025-05-19T06:12:27.037522Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Secțiunea 4.1 - Rețea neuronală convoluțională (CNN) în Keras/TensorFlow, destinată clasificării binare","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf  # Importă biblioteca TensorFlow pentru machine learning și deep learning\nfrom tensorflow import keras  # Importă modulul keras din TensorFlow, o interfață simplificată pentru rețele neuronale\nfrom tensorflow.keras import layers  # Importă modulul pentru definirea straturilor rețelei neuronale (Dense, Conv2D etc.)\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping  \n# callback1: ReduceLROnPlateau scade rata de învățare dacă performanța stagnează\n# callback2: EarlyStopping oprește antrenamentul dacă nu mai există îmbunătățiri (pentru a evita overfitting-ul)\n\n\n## deep learning model\nmodel = keras.models.Sequential() #creez modelul secvențial, strat cu strat\n\n#filters=filtrele\n#kernel_size=(A,B): filtrele sunt de A canale pe B eșantioane\n#layers.Conv2D - convolutie\n#pooling - operație de reducere a dimensiunii datelor pastrand cele mai importante caracteristici\n\nmodel.add(layers.Conv2D(filters=64, kernel_size=(2, 4), padding='same', activation='relu', input_shape=X_train.shape[1:]))\nmodel.add(layers.Conv2D(filters=64, kernel_size=(2, 4), strides=(1, 2),padding='same', activation='relu'))\nmodel.add(layers.MaxPooling2D((1, 2)))\n\nmodel.add(layers.Conv2D(filters=128, kernel_size=(2, 4), padding='same', activation='relu'))\nmodel.add(layers.Conv2D(filters=128, kernel_size=(2, 4), strides=(1, 2), padding='same', activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\n\nmodel.add(layers.Conv2D(filters=256, kernel_size=(4, 4), padding='same', activation='relu'))\nmodel.add(layers.Conv2D(filters=256, kernel_size=(4, 4), strides=(1, 2), padding='same', activation='relu'))\nmodel.add(layers.MaxPooling2D((1, 2)))\n\nmodel.add(layers.GlobalAveragePooling2D())\n#model.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dropout(0.25))\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dropout(0.25))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()\n\n#-------------------------------------------------------------------------------\nfrom keras.utils import plot_model\n#plot_model(model, show_shapes=True, to_file='model.png')\nplot_model(model, show_shapes=True, dpi=70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:12:30.139802Z","iopub.execute_input":"2025-05-19T06:12:30.140561Z","iopub.status.idle":"2025-05-19T06:12:31.966998Z","shell.execute_reply.started":"2025-05-19T06:12:30.140532Z","shell.execute_reply":"2025-05-19T06:12:31.966228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LEARNING_RATE = 1e-4\nOPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n\nmodel.compile(optimizer=OPTIMIZER, loss='binary_crossentropy', metrics=['accuracy'])\n\n# callbacks\nVERBOSE=1\n#lr = ReduceLROnPlateau(monitor='val_loss', factor=0.75, patience=5, verbose=VERBOSE, min_le=1e-8)\nes = EarlyStopping(monitor='val_loss', patience=20, verbose=VERBOSE, mode='auto', restore_best_weights=True)\n\ncallbacks = [es]\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:12:38.778221Z","iopub.execute_input":"2025-05-19T06:12:38.778516Z","iopub.status.idle":"2025-05-19T06:12:38.793960Z","shell.execute_reply.started":"2025-05-19T06:12:38.778495Z","shell.execute_reply":"2025-05-19T06:12:38.793425Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Secțiunea 4.2 - Antrenarea modelului","metadata":{}},{"cell_type":"code","source":"hist = model.fit(\n    x=X_train, y=y_train,\n    validation_data=(X_test, y_test),\n    epochs=200,\n    batch_size=256,\n    callbacks=callbacks\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:12:53.770521Z","iopub.execute_input":"2025-05-19T06:12:53.770792Z","iopub.status.idle":"2025-05-19T06:44:13.172203Z","shell.execute_reply.started":"2025-05-19T06:12:53.770772Z","shell.execute_reply":"2025-05-19T06:44:13.171644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Creez un DataFrame din istoricul antrenării\nhistory_df = pd.DataFrame(hist.history)\n\n# Salvez ca fișier CSV\nhistory_df.to_csv('training_history_rs2023.csv', index=False)\n\nhistory_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:44:49.328650Z","iopub.execute_input":"2025-05-19T06:44:49.328925Z","iopub.status.idle":"2025-05-19T06:44:49.352001Z","shell.execute_reply.started":"2025-05-19T06:44:49.328904Z","shell.execute_reply":"2025-05-19T06:44:49.351476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save('CHB_MIT_sz_detec_rs2023.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:45:12.779853Z","iopub.execute_input":"2025-05-19T06:45:12.780389Z","iopub.status.idle":"2025-05-19T06:45:12.866797Z","shell.execute_reply.started":"2025-05-19T06:45:12.780366Z","shell.execute_reply":"2025-05-19T06:45:12.866239Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Secțiunea 4.3 - Grafice pentru evoluția pierderii și acuratețe","metadata":{}},{"cell_type":"code","source":"#imediat dupa antrenare\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\n\nax[0].plot(hist.history['loss'], label='loss')\nax[0].plot(hist.history['val_loss'], label='val_loss')\nax[0].set_xlabel('epoch')\nax[0].set_ylabel('loss')\nax[0].axvline(x=es.best_epoch, label='early stopping', color='tab:red', alpha=0.5)\nr = .2\ntemp_y = r*min(hist.history['loss'])+(1-r)*max(hist.history['loss'])\nax[0].annotate(' early stopping:\\n best epoch', xy=(es.best_epoch, temp_y))\nax[0].set_title('Loss')\nax[0].legend()\n\nax[1].plot(hist.history['accuracy'], label='accuracy')\nax[1].plot(hist.history['val_accuracy'], label='val_accuracy')\nax[1].set_xlabel('epoch')\nax[1].set_ylabel('accuracy')\nr = .8\ntemp_y = r*min(hist.history['accuracy'])+(1-r)*max(hist.history['accuracy'])\nax[1].axvline(x=es.best_epoch, label='early stopping', color='tab:red', alpha=0.5)\nax[1].annotate(' early stopping:\\n best epoch', xy=(es.best_epoch, temp_y))\nax[1].set_title('Accuracy')\nax[1].legend()\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:54:48.894624Z","iopub.execute_input":"2025-05-19T06:54:48.895213Z","iopub.status.idle":"2025-05-19T06:54:49.232110Z","shell.execute_reply.started":"2025-05-19T06:54:48.895186Z","shell.execute_reply":"2025-05-19T06:54:49.231465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#preia antrenarea din memorie\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Încarcă istoricul din fișierul CSV\ndf = pd.read_csv('/kaggle/input/model-history-training-rs2023/training_history_rs2023.csv')\n\n# Adăugăm o coloană 'epoch' care reprezintă indicele fiecărui rând\ndf['epoch'] = df.index\n\n# Creăm un dicționar cu valorile necesare pentru plot\nhist = {\n    'loss': df['loss'].values,\n    'val_loss': df['val_loss'].values,\n    'accuracy': df['accuracy'].values,\n    'val_accuracy': df['val_accuracy'].values\n}\n\n# Configurare plot\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\n\n# Plot pentru Loss\nax[0].plot(hist['loss'], label='loss')\nax[0].plot(hist['val_loss'], label='val_loss')\nax[0].set_xlabel('epoch')\nax[0].set_ylabel('loss')\n\n# Mark the early stopping epoch (care este momentul cu min. val_loss)\nes_best_epoch = df['val_loss'].idxmin()  # Epoca cu valoarea minimă pentru 'val_loss'\nax[0].axvline(x=es_best_epoch, label='early stopping', color='tab:red', alpha=0.5)\nr = .2\ntemp_y = r*min(hist['loss'])+(1-r)*max(hist['loss'])\nax[0].annotate(' early stopping:\\n best epoch', xy=(es_best_epoch, temp_y))\nax[0].set_title('Loss')\nax[0].legend()\n\n# Plot pentru Accuracy\nax[1].plot(hist['accuracy'], label='accuracy')\nax[1].plot(hist['val_accuracy'], label='val_accuracy')\nax[1].set_xlabel('epoch')\nax[1].set_ylabel('accuracy')\n\n# Mark the early stopping epoch (care este momentul cu min. val_loss)\nr = .8\ntemp_y = r*min(hist['accuracy'])+(1-r)*max(hist['accuracy'])\nax[1].axvline(x=es_best_epoch, label='early stopping', color='tab:red', alpha=0.5)\nax[1].annotate(' early stopping:\\n best epoch', xy=(es_best_epoch, temp_y))\nax[1].set_title('Accuracy')\nax[1].legend()\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:55:52.283635Z","iopub.execute_input":"2025-05-19T06:55:52.284233Z","iopub.status.idle":"2025-05-19T06:55:52.613934Z","shell.execute_reply.started":"2025-05-19T06:55:52.284211Z","shell.execute_reply":"2025-05-19T06:55:52.613297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#run the model without training\nimport tensorflow as tf\nfrom tensorflow import keras\nmodel = keras.models.load_model('/kaggle/input/cnn_eeg_seizure_model/tensorflow2/default/1/CHB_MIT_sz_detec.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-16T11:49:54.042598Z","iopub.execute_input":"2025-05-16T11:49:54.042870Z","iopub.status.idle":"2025-05-16T11:50:08.560775Z","shell.execute_reply.started":"2025-05-16T11:49:54.042850Z","shell.execute_reply":"2025-05-16T11:50:08.559937Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#run the model without training\nimport tensorflow as tf\nfrom tensorflow import keras\nmodel = keras.models.load_model('/kaggle/input/cnn_eeg_model_rs2023/tensorflow2/default/1/CHB_MIT_sz_detec_rs2023.h5')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Secțiunea 5 - Evaluarea Modelului","metadata":{}},{"cell_type":"code","source":"def sampling_data_pred(f, verbose=True):\n    list_signals = []\n    list_is_sz = []\n    #n_sample = 40\n    if verbose==True:\n        print('{}: Reading. '.format(f))\n    temp_edf =  mne.io.read_raw_edf(f)\n    temp_labels = temp_edf.ch_names\n    if sum([any([0 if re.match(c, l)==None else 1 for l in temp_edf.ch_names]) for c in ch_labels])==len(ch_labels):\n        ch_mapping = {sorted([l for l in temp_edf.ch_names if re.match(c, l)!=None ])[0]:c for c in ch_labels}\n        temp_edf.rename_channels(ch_mapping)\n        #temp_edf = temp_edf.pick(ch_labels)\n\n        temp_is_sz = np.zeros((temp_edf.n_times,))\n        temp_signals = temp_edf.get_data(picks=ch_labels)*1e6\n\n        if os.path.exists(f+'.seizures'):\n            if verbose==True:\n                print('sz exists.', end=' ')\n            temp_annotation = wfdb.rdann(f, 'seizures')\n            for i in range(int(temp_annotation.sample.size/2)):\n                temp_is_sz[temp_annotation.sample[i*2]:temp_annotation.sample[i*2+1]]=1\n\n        temp_len = temp_edf.n_times\n\n        time_window = 8\n        time_step = 4\n        fs = int(1/(temp_edf.times[1]-temp_edf.times[0]))\n        step_window = time_window*fs\n        step = time_step*fs\n\n        # sampling all signals\n        temp_array_signals = np.array([temp_signals[:, i*step:i*step+step_window] for i in range((temp_len-step_window)//step)])\n        temp_is_sz_ind = np.array([temp_is_sz[i*step:i*step+step_window].sum()/step_window for i in range((temp_len-step_window)//step)])\n    else:\n        if verbose==True:\n            print('EEG {}: Not appropriate channel labels. Reading skipped.'.format(n))\n\n    return temp_array_signals, temp_is_sz_ind","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:56:40.260334Z","iopub.execute_input":"2025-05-19T06:56:40.260928Z","iopub.status.idle":"2025-05-19T06:56:40.268826Z","shell.execute_reply.started":"2025-05-19T06:56:40.260906Z","shell.execute_reply":"2025-05-19T06:56:40.268129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mne.set_log_level(verbose='ERROR') #show only error messages","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:56:44.107337Z","iopub.execute_input":"2025-05-19T06:56:44.107641Z","iopub.status.idle":"2025-05-19T06:56:44.111721Z","shell.execute_reply.started":"2025-05-19T06:56:44.107619Z","shell.execute_reply":"2025-05-19T06:56:44.110934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# reading files and prediction\n\nlist_pred = []\nlist_true = []\n\nfor f in tqdm.tqdm(files_test):\n    array_signals, array_is_sz = sampling_data_pred(f, verbose=False)\n    array_signals = array_signals[:, :, ::2, np.newaxis]\n    \n    list_pred.append(model.predict(array_signals, verbose=0))\n    list_true.append(array_is_sz)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:56:48.644990Z","iopub.execute_input":"2025-05-19T06:56:48.645723Z","iopub.status.idle":"2025-05-19T07:03:41.022760Z","shell.execute_reply.started":"2025-05-19T06:56:48.645698Z","shell.execute_reply":"2025-05-19T07:03:41.022166Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Arata fisierul si predictiile","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n\n# # Aplatizează predicțiile și etichetele într-un singur array\n# y_pred_all = np.concatenate(list_pred)\n# y_true_all = np.concatenate(list_true)\n\n# # Aplatizare în cazul în care predicțiile sunt în format coloană\n# if y_pred_all.ndim > 1 and y_pred_all.shape[1] == 1:\n#     y_pred_all = y_pred_all.flatten()\n\n# # Creează DataFrame cu primele 30 de exemple\n# df = pd.DataFrame({\n#     'Predicted': y_pred_all[:50000],\n#     'Actual': y_true_all[:50000]\n# })\n\n# # Adaugă eticheta binară (0 sau 1) pe baza unui prag de 0.5\n# df['Predicted_Label'] = (df['Predicted'] >= 0.5).astype(int)\n\n# # Salvează tabelul într-un fișier CSV\n# df.to_csv('model_predictions_sample.csv', index=False)\n\n# # Confirmare\n# print(\"Fișierul 'model_predictions_sample.csv' a fost salvat.\")\n#-------------------------------------------------------------------------------------------\nimport numpy as np\nimport pandas as pd\n\n# Aplatizează predicțiile și etichetele\ny_pred_all = np.concatenate(list_pred)\ny_true_all = np.concatenate(list_true)\n\n# Aplatizare dacă e necesar\nif y_pred_all.ndim > 1 and y_pred_all.shape[1] == 1:\n    y_pred_all = y_pred_all.flatten()\n\n# Creează DataFrame complet\ndf_all = pd.DataFrame({\n    'Predicted': y_pred_all,\n    'Actual': y_true_all\n})\n\n# Filtrare: doar segmente cu criză reală\ndf_sz_only = df_all[df_all['Actual'] > 0]\n\n# Adaugă eticheta binară de predicție\ndf_sz_only['Predicted_Label'] = (df_sz_only['Predicted'] >= 0.5).astype(int)\n\n# Salvează fișierul CSV\ndf_sz_only.to_csv('model_predictions_only_seizures_rs2023.csv', index=False)\n\nprint(\"Fișierul 'model_predictions_only_seizures_rs2023.csv' a fost salvat.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:04:23.405779Z","iopub.execute_input":"2025-05-19T07:04:23.406058Z","iopub.status.idle":"2025-05-19T07:04:23.419666Z","shell.execute_reply.started":"2025-05-19T07:04:23.406038Z","shell.execute_reply":"2025-05-19T07:04:23.418886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Număr total segmente cu crize:\", np.sum(y_true_all > 0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:05:36.438184Z","iopub.execute_input":"2025-05-19T07:05:36.438919Z","iopub.status.idle":"2025-05-19T07:05:36.443439Z","shell.execute_reply.started":"2025-05-19T07:05:36.438894Z","shell.execute_reply":"2025-05-19T07:05:36.442689Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Secțiunea 5.1 - Precision, Recall, F1-score","metadata":{}},{"cell_type":"code","source":"print(f\"Număr total de crize (în etichetele reale): {np.sum(y_true_all)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:04:47.772939Z","iopub.execute_input":"2025-05-19T07:04:47.773208Z","iopub.status.idle":"2025-05-19T07:04:47.777696Z","shell.execute_reply.started":"2025-05-19T07:04:47.773188Z","shell.execute_reply":"2025-05-19T07:04:47.776922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Dimensiune y_true_all: {y_true_all.shape}\")\nprint(f\"Dimensiune y_pred_all: {y_pred_all.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:05:14.849180Z","iopub.execute_input":"2025-05-19T07:05:14.849489Z","iopub.status.idle":"2025-05-19T07:05:14.853861Z","shell.execute_reply.started":"2025-05-19T07:05:14.849464Z","shell.execute_reply":"2025-05-19T07:05:14.853262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Valori unice în y_true_all:\", np.unique(y_true_all))\nprint(\"Dimensiune y_true_all:\", y_true_all.shape)\nprint(\"Suma elementelor:\", np.sum(y_true_all))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:05:18.854761Z","iopub.execute_input":"2025-05-19T07:05:18.855042Z","iopub.status.idle":"2025-05-19T07:05:18.870984Z","shell.execute_reply.started":"2025-05-19T07:05:18.855020Z","shell.execute_reply":"2025-05-19T07:05:18.870302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn import metrics\n\n# threshold = 0.5\nreport = metrics.classification_report(np.concatenate(list_true)>0, np.concatenate(list_pred)>.5)\nprint(report)\n\n# threshold = 0.9\nreport = metrics.classification_report(np.concatenate(list_true)>0, np.concatenate(list_pred)>.9)\nprint(report)\n\n# threshold = 0.4\nreport = metrics.classification_report(np.concatenate(list_true)>0, np.concatenate(list_pred)>.4)\nprint(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:05:39.328301Z","iopub.execute_input":"2025-05-19T07:05:39.328976Z","iopub.status.idle":"2025-05-19T07:05:39.923722Z","shell.execute_reply.started":"2025-05-19T07:05:39.328950Z","shell.execute_reply":"2025-05-19T07:05:39.923116Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Secțiunea 5.2 - Curba ROC","metadata":{}},{"cell_type":"code","source":"roc = metrics.roc_curve(np.concatenate(list_true)>0, np.concatenate(list_pred))\nauc = metrics.roc_auc_score(np.concatenate(list_true)>0, np.concatenate(list_pred))\nplt.figure(figsize=(4, 4))\nplt.plot(roc[0][np.argmin(np.abs(roc[2]-1)):], roc[1][np.argmin(np.abs(roc[2]-1)):])\nplt.xlabel('FPR: false positive rate')\nplt.ylabel('TPR: true positive rate')\nplt.title('ROC curve: AUC score = {:.2f}'.format(auc))\n\nth = [.1, .2, .5, .9, .95, 1.]\nind = [np.argmin(np.abs(roc[2]-l)) for l in th]\nplt.scatter(roc[0][ind], roc[1][ind], s=15)\nfor i, l in enumerate(ind):\n    plt.annotate(\"{}\".format(th[i]), xy=(roc[0][l], roc[1][l]))\n#plt.plot([0, 1, 1, 0, 0], [0, 0, 1, 1, 0], color='black', linewidth=1)\nplt.ylim(-.05, 1.05)\nplt.xlim(-.05, 1.05)\nplt.grid()\n#plt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:05:48.582871Z","iopub.execute_input":"2025-05-19T07:05:48.583477Z","iopub.status.idle":"2025-05-19T07:05:48.839477Z","shell.execute_reply.started":"2025-05-19T07:05:48.583453Z","shell.execute_reply":"2025-05-19T07:05:48.838690Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Secțiunea 5.3 - Matricea de confuzie","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\ny_pred_all = np.concatenate(list_pred)\ny_true_all = np.concatenate(list_true)\n\nif y_pred_all.ndim > 1 and y_pred_all.shape[1] == 1:\n    y_pred_all = y_pred_all.flatten()\n\ny_pred_labels = (y_pred_all >= 0.5).astype(int)\ny_true_labels = (y_true_all >= 0.5).astype(int)\n\ncm = confusion_matrix(y_true_labels, y_pred_labels)\n\ncm_df = pd.DataFrame(cm, index=[\"Non-Criza\", \"Criza\"], columns=[\"Non-Criza\", \"Criza\"])\n\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', cbar=False)\nplt.title(\"Matricea de Confuzie\")\nplt.xlabel(\"Predicții\")\nplt.ylabel(\"Etichete Reale\")\nplt.show()\n\nTN, FP, FN, TP = cm.ravel()\n\nprint(f\"Număr de non-crize reale (TN): {TN}\")\nprint(f\"Număr de crize reale (TP): {TP}\")\nprint(f\"Număr de non-crize prezise greșit (FP): {FP}\")\nprint(f\"Număr de crize prezise greșit (FN): {FN}\")\n\nprint(\"\\nExplicație:\")\nprint(f\"Din totalul crizelor reale ({TP + FN}), modelul a prezis corect {TP} crize și a prezis greșit {FN}.\")\nprint(f\"Din totalul non-crizelor reale ({TN + FP}), modelul a prezis corect {TN} non-crize și a prezis greșit {FP}.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:05:56.681766Z","iopub.execute_input":"2025-05-19T07:05:56.682515Z","iopub.status.idle":"2025-05-19T07:05:57.124314Z","shell.execute_reply.started":"2025-05-19T07:05:56.682490Z","shell.execute_reply":"2025-05-19T07:05:57.123605Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Secțiunea 5.4 - Seizure detection point","metadata":{}},{"cell_type":"code","source":"for i, f in enumerate(files_test):\n    if os.path.exists(f+'.seizures'):\n        print('Index = {} has seizures: {}'.format(i, f))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:06:34.660541Z","iopub.execute_input":"2025-05-19T07:06:34.661034Z","iopub.status.idle":"2025-05-19T07:06:34.680651Z","shell.execute_reply.started":"2025-05-19T07:06:34.661013Z","shell.execute_reply":"2025-05-19T07:06:34.679931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def moving_ave(a, n):\n    if len(a.shape)!=1:\n        print('Not 1 dimension array. return nothing.')\n        return\n    temp = np.zeros(a.size-n)\n    for i in range(n):\n        temp = temp+a[i:-n+i]\n    temp = temp/n\n    \n    return temp\n\n\n# get signals and labels from test data.\nn=100\narray_signals, array_is_sz = sampling_data_pred(files_test[n])\n\n# preprocess\narray_signals=array_signals[:, :, ::2, np.newaxis]\n\n# use deep learning model\npred = model.predict(array_signals)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:06:38.052518Z","iopub.execute_input":"2025-05-19T07:06:38.052792Z","iopub.status.idle":"2025-05-19T07:06:41.100118Z","shell.execute_reply.started":"2025-05-19T07:06:38.052772Z","shell.execute_reply":"2025-05-19T07:06:41.099600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time_window = 8\ntime_step = 4\nmv_win = 3\n\nfig, ax = plt.subplots(figsize=(12, 2))\n\nax.plot(np.arange(pred.size)*time_step, pred.flatten(), alpha=0.7, label='deep learning model pred')\nax.plot(np.arange(pred.size)*time_step, array_is_sz, alpha=.7, label='True label')\n\npred_moving_ave = moving_ave(pred.flatten(), mv_win)\npred_peaks, _ = find_peaks(pred_moving_ave, height=.95, distance=6)\nax.plot(np.arange(pred.size-mv_win)*time_step, pred_moving_ave,\n        alpha=.9, label='pred - moving ave', color='tab:pink', zorder=0)\nax.scatter(pred_peaks*time_step, pred_moving_ave[pred_peaks], s=20, color='tab:red')\n\nax.set_xlabel('time (s)')\nax.set_ylabel('p')\nax.set_xlim(0, pred.size*time_step+500)\nax.legend(loc='upper right')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:06:43.629840Z","iopub.execute_input":"2025-05-19T07:06:43.630504Z","iopub.status.idle":"2025-05-19T07:06:43.779013Z","shell.execute_reply.started":"2025-05-19T07:06:43.630480Z","shell.execute_reply":"2025-05-19T07:06:43.778291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if pred_peaks.size==0:\n    print('No seizure detected.')\nelse:\n    f = files_test[n]\n    temp_edf =  mne.io.read_raw_edf(f)\n    temp_labels = temp_edf.ch_names\n    if sum([any([0 if re.match(c, l)==None else 1 for l in temp_edf.ch_names]) for c in ch_labels])==len(ch_labels):\n        ch_mapping = {sorted([l for l in temp_edf.ch_names if re.match(c, l)!=None ])[0]:c for c in ch_labels}\n        temp_edf.rename_channels(ch_mapping)\n        #temp_edf = temp_edf.pick(ch_labels)\n\n        temp_is_sz = np.zeros((temp_edf.n_times,))\n        temp_signals = temp_edf.get_data(picks=ch_labels)*1e6\n\n    fs = int(1/(temp_edf.times[1]-temp_edf.times[0]))\n    for n_peak in range(pred_peaks.size):\n        ind_peak = pred_peaks[n_peak]*time_step*fs\n        backward_steps = 30*fs\n        forward_steps = 15*fs\n        vertical_width=500\n\n        fig, ax = plt.subplots(figsize=(10, 6))\n        for i in range(temp_signals.shape[0]):\n            ax.plot(np.arange(ind_peak-backward_steps, ind_peak+forward_steps)/fs,\n                    temp_signals[i, ind_peak-backward_steps:ind_peak+forward_steps]+i*vertical_width, linewidth=0.5, color='tab:blue')\n            ax.annotate(ch_labels[i], xy=((ind_peak-backward_steps)/fs, i*vertical_width))\n        ax.axvline(x=ind_peak/fs, color='tab:red', alpha=0.5, label='Seizure detection point')\n        ax.invert_yaxis()\n        ax.legend(loc='upper right')\n        plt.show()\n    #ax.set_xlim(0, 8)\n\n    temp_edf.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:06:50.365193Z","iopub.execute_input":"2025-05-19T07:06:50.365903Z","iopub.status.idle":"2025-05-19T07:06:51.520669Z","shell.execute_reply.started":"2025-05-19T07:06:50.365880Z","shell.execute_reply":"2025-05-19T07:06:51.519985Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# NOT USEFULL YET","metadata":{}},{"cell_type":"markdown","source":"**HEATMAPS**  **training part**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm import tqdm  # pentru bara de progres\n\n# Creează foldere pentru imaginile de train\nos.makedirs('/kaggle/working/heatmaps/train/criza', exist_ok=True)\nos.makedirs('/kaggle/working/heatmaps/train/non_criza', exist_ok=True)\n\n# Funcție care salvează heatmap-ul\ndef save_heatmap(signal, path):\n    plt.figure(figsize=(4, 4))\n    plt.axis('off')\n    plt.pcolormesh(signal, cmap='gray')\n    plt.gca().invert_yaxis()\n    plt.tight_layout(pad=0)\n    plt.savefig(path, bbox_inches='tight', pad_inches=0)\n    plt.close()\n\n# Salvează imaginile de antrenare\nfor idx in tqdm(range(X_train.shape[0]), desc=\"Generăm heatmap-uri pentru train\"):\n    label = 'criza' if y_train[idx] else 'non_criza'\n    save_path = f'/kaggle/working/heatmaps/train/{label}/{idx}.png'\n    save_heatmap(X_train[idx, :, :], save_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\nimport os\n\n# Setează calea către folderul care conține imaginile\nfolder_path = '/kaggle/working/heatmaps/train'\n\n# Numele fișierului zip\nzip_file = '/kaggle/working/heatmaps_train.zip'\n\n# Creează un fișier zip\nwith zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    # Parcurge folderele din directory\n    for root, dirs, files in os.walk(folder_path):\n        for file in files:\n            # Adaugă fiecare fișier .png la arhivă\n            if file.endswith('.png'):\n                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), folder_path))\n\nprint(f'Fișierul zip a fost creat la {zip_file}')\n\nimport shutil\n\n# Muta arhiva într-un loc accesibil pentru download\nshutil.move(zip_file, '/kaggle/working/heatmaps_train.zip')\n\n# Link de descărcare\nfrom IPython.display import FileLink\n\n# Crează un link de descărcare\nFileLink(r'/kaggle/working/heatmaps_train.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\n\nimage_paths = []\nlabels = []\n\nbase_path = '/kaggle/working/heatmaps/train'\n\nfor label in ['criza', 'non_criza']:\n    full_path = os.path.join(base_path, label)\n    for fname in os.listdir(full_path):\n        if fname.endswith('.png'):\n            image_paths.append(os.path.join(full_path, fname))\n            labels.append(label)\n\ntrain_df = pd.DataFrame({\n    'image_path': image_paths,\n    'label': labels\n})\n\ntrain_df.to_csv('/kaggle/working/train_dataset.csv', index=False)\n\nprint(\"CSV-ul a fost creat pe baza imaginilor existente.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\n# Creează foldere pentru imaginile de train\nos.makedirs('/kaggle/working/heatmaps/train/criza', exist_ok=True)\nos.makedirs('/kaggle/working/heatmaps/train/non_criza', exist_ok=True)\n\n# Funcție care salvează heatmap-ul\ndef save_heatmap(signal, path):\n    plt.figure(figsize=(4, 4))\n    plt.axis('off')\n    plt.pcolormesh(signal, cmap='gray')\n    plt.gca().invert_yaxis()\n    plt.tight_layout(pad=0)\n    plt.savefig(path, bbox_inches='tight', pad_inches=0)\n    plt.close()\n\n# Creăm o listă pentru căile fișierelor și etichetele corespunzătoare\nimage_paths = []\nlabels = []\n\n# Salvează imaginile de antrenare\nfor idx in tqdm(range(X_train.shape[0]), desc=\"Generăm heatmap-uri pentru train\"):\n    label = 'criza' if y_train[idx] else 'non_criza'\n    save_path = f'/kaggle/working/heatmaps/train/{label}/{idx}.png'\n    save_heatmap(X_train[idx, :, :], save_path)\n    \n    # Adăugăm calea fișierului și eticheta în liste\n    image_paths.append(save_path)\n    labels.append(label)\n\n# Creăm un DataFrame din listele de căi și etichete\ntrain_df = pd.DataFrame({\n    'image_path': image_paths,\n    'label': labels\n})\n\n# Salvăm DataFrame-ul ca fișier CSV\ntrain_df.to_csv('/kaggle/working/train_dataset.csv', index=False)\n\nprint(\"Dataset-ul pentru train a fost salvat ca fișier CSV.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import numpy as np\n# import matplotlib.pyplot as plt\n# import os\n# from tqdm import tqdm  # pentru bara de progres\n\n# # Creează foldere pentru imagini\n# os.makedirs('/kaggle/working/heatmaps/train/criza', exist_ok=True)\n# os.makedirs('/kaggle/working/heatmaps/train/non_criza', exist_ok=True)\n# os.makedirs('/kaggle/working/heatmaps/test/criza', exist_ok=True)\n# os.makedirs('/kaggle/working/heatmaps/test/non_criza', exist_ok=True)\n\n# # Funcție care salvează heatmap-ul\n# def save_heatmap(signal, path):\n#     plt.figure(figsize=(4, 4))\n#     plt.axis('off')\n#     plt.pcolormesh(signal, cmap='gray')  # poți schimba cmap dacă vrei alt efect\n#     plt.gca().invert_yaxis()\n#     plt.tight_layout(pad=0)\n#     plt.savefig(path, bbox_inches='tight', pad_inches=0)\n#     plt.close()\n\n# # Salvează imaginile de antrenare\n# for idx in tqdm(range(X_train.shape[0]), desc=\"Generăm heatmap-uri pentru train\"):\n#     label = 'criza' if y_train[idx] else 'non_criza'\n#     save_path = f'/kaggle/working/heatmaps/train/{label}/{idx}.png'\n#     save_heatmap(X_train[idx, :, :], save_path)\n\n# # Salvează imaginile de testare\n# for idx in tqdm(range(X_test.shape[0]), desc=\"Generăm heatmap-uri pentru test\"):\n#     label = 'criza' if y_test[idx] else 'non_criza'\n#     save_path = f'/kaggle/working/heatmaps/test/{label}/{idx}.png'\n#     save_heatmap(X_test[idx, :, :], save_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#DE LA CHAT GPT \n#from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n# from tensorflow.keras.optimizers import Adam\n\n# # Definirea modelului\n# model = Sequential()\n\n# # 1. Primul strat convoluțional\n# model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(18, 2048, 1)))\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D((2, 2)))\n\n# # 2. Al doilea strat convoluțional\n# model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D((2, 2)))\n\n# # 3. Al treilea strat convoluțional\n# model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n# model.add(BatchNormalization())\n# model.add(MaxPooling2D((2, 2)))\n\n# # 4. Flatten + Dense layers\n# model.add(Flatten())\n\n# model.add(Dense(128, activation='relu'))\n# model.add(Dropout(0.5))\n\n# # 5. Stratul final - clasificare binară\n# model.add(Dense(1, activation='sigmoid'))\n\n# # Compilarea modelului\n# model.compile(\n#     optimizer=Adam(learning_rate=0.001),\n#     loss='binary_crossentropy',\n#     metrics=['accuracy']\n# )\n\n# # Rezumat model\n# model.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}